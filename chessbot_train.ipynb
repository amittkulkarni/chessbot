{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac5b674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T04:21:23.007716Z",
     "iopub.status.busy": "2026-01-25T04:21:23.007473Z",
     "iopub.status.idle": "2026-01-25T04:21:30.695137Z",
     "shell.execute_reply": "2026-01-25T04:21:30.694397Z"
    },
    "papermill": {
     "duration": 7.692536,
     "end_time": "2026-01-25T04:21:30.696934",
     "exception": false,
     "start_time": "2026-01-25T04:21:23.004398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-chess\r\n",
      "  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\r\n",
      "Collecting chess<2,>=1 (from python-chess)\r\n",
      "  Downloading chess-1.11.2.tar.gz (6.1 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\r\n",
      "Building wheels for collected packages: chess\r\n",
      "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=f004a3b000e6afef78592c2500f65017ebe139f883d98c9e0b1f83b33e9d67e0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/1f/4e/8f4300f7dd554eb8de70ddfed96e94d3d030ace10c5b53d447\r\n",
      "Successfully built chess\r\n",
      "Installing collected packages: chess, python-chess\r\n",
      "Successfully installed chess-1.11.2 python-chess-1.999\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63848c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T04:21:30.702562Z",
     "iopub.status.busy": "2026-01-25T04:21:30.702313Z",
     "iopub.status.idle": "2026-01-25T12:11:32.183688Z",
     "shell.execute_reply": "2026-01-25T12:11:32.182756Z"
    },
    "papermill": {
     "duration": 28201.495628,
     "end_time": "2026-01-25T12:11:32.194576",
     "exception": false,
     "start_time": "2026-01-25T04:21:30.698948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuration:\n",
      "   ‚Ä¢ Device: cuda (2 GPUs)\n",
      "   ‚Ä¢ Dataset Root: /kaggle/input/chess-stockfish-data\n",
      "‚ö° Enabling DataParallel on 2 GPUs\n",
      "üîÑ Resuming from /kaggle/input/teacher-model-weights/resnet20_epoch2.pth\n",
      "üìÇ Found 5 data folders. Scanning for files...\n",
      "‚úÖ Total Training Files Found: 36\n",
      "\n",
      "--- Starting Epoch 3/3 ---\n",
      "E3|B100 >> P:1.370 | V:0.032\n",
      "E3|B200 >> P:1.388 | V:0.032\n",
      "E3|B300 >> P:1.321 | V:0.039\n",
      "E3|B400 >> P:1.248 | V:0.036\n",
      "E3|B500 >> P:1.364 | V:0.031\n",
      "E3|B600 >> P:1.272 | V:0.035\n",
      "E3|B700 >> P:1.289 | V:0.022\n",
      "E3|B800 >> P:1.278 | V:0.034\n",
      "E3|B900 >> P:1.338 | V:0.030\n",
      "E3|B1000 >> P:1.293 | V:0.032\n",
      "E3|B1100 >> P:1.306 | V:0.027\n",
      "E3|B1200 >> P:1.361 | V:0.030\n",
      "E3|B1300 >> P:1.414 | V:0.030\n",
      "E3|B1400 >> P:1.393 | V:0.031\n",
      "E3|B1500 >> P:1.351 | V:0.038\n",
      "E3|B1600 >> P:1.238 | V:0.029\n",
      "E3|B1700 >> P:1.314 | V:0.029\n",
      "E3|B1800 >> P:1.283 | V:0.026\n",
      "E3|B1900 >> P:1.332 | V:0.027\n",
      "E3|B2000 >> P:1.276 | V:0.034\n",
      "E3|B2100 >> P:1.281 | V:0.032\n",
      "E3|B2200 >> P:1.395 | V:0.033\n",
      "E3|B2300 >> P:1.236 | V:0.026\n",
      "E3|B2400 >> P:1.171 | V:0.038\n",
      "E3|B2500 >> P:1.320 | V:0.027\n",
      "E3|B2600 >> P:1.217 | V:0.025\n",
      "E3|B2700 >> P:1.201 | V:0.036\n",
      "E3|B2800 >> P:1.128 | V:0.030\n",
      "E3|B2900 >> P:1.165 | V:0.027\n",
      "E3|B3000 >> P:1.203 | V:0.024\n",
      "E3|B3100 >> P:1.188 | V:0.036\n",
      "E3|B3200 >> P:1.189 | V:0.037\n",
      "E3|B3300 >> P:1.230 | V:0.035\n",
      "E3|B3400 >> P:1.169 | V:0.034\n",
      "E3|B3500 >> P:1.242 | V:0.028\n",
      "E3|B3600 >> P:1.195 | V:0.028\n",
      "E3|B3700 >> P:1.188 | V:0.031\n",
      "E3|B3800 >> P:1.368 | V:0.031\n",
      "E3|B3900 >> P:1.285 | V:0.038\n",
      "E3|B4000 >> P:1.264 | V:0.029\n",
      "E3|B4100 >> P:1.249 | V:0.030\n",
      "E3|B4200 >> P:1.225 | V:0.039\n",
      "E3|B4300 >> P:1.301 | V:0.028\n",
      "E3|B4400 >> P:1.157 | V:0.025\n",
      "E3|B4500 >> P:1.363 | V:0.020\n",
      "E3|B4600 >> P:1.209 | V:0.041\n",
      "E3|B4700 >> P:1.396 | V:0.031\n",
      "E3|B4800 >> P:1.336 | V:0.038\n",
      "E3|B4900 >> P:1.272 | V:0.029\n",
      "E3|B5000 >> P:1.386 | V:0.024\n",
      "E3|B5100 >> P:1.409 | V:0.032\n",
      "E3|B5200 >> P:1.265 | V:0.027\n",
      "E3|B5300 >> P:1.260 | V:0.028\n",
      "E3|B5400 >> P:1.301 | V:0.028\n",
      "E3|B5500 >> P:1.230 | V:0.039\n",
      "E3|B5600 >> P:1.296 | V:0.037\n",
      "E3|B5700 >> P:1.535 | V:0.032\n",
      "E3|B5800 >> P:1.429 | V:0.026\n",
      "E3|B5900 >> P:1.433 | V:0.035\n",
      "E3|B6000 >> P:1.414 | V:0.032\n",
      "E3|B6100 >> P:1.367 | V:0.037\n",
      "E3|B6200 >> P:1.368 | V:0.019\n",
      "E3|B6300 >> P:1.324 | V:0.031\n",
      "E3|B6400 >> P:1.401 | V:0.025\n",
      "E3|B6500 >> P:1.316 | V:0.036\n",
      "E3|B6600 >> P:1.315 | V:0.030\n",
      "E3|B6700 >> P:1.238 | V:0.024\n",
      "E3|B6800 >> P:1.361 | V:0.027\n",
      "E3|B6900 >> P:1.460 | V:0.032\n",
      "E3|B7000 >> P:1.390 | V:0.028\n",
      "E3|B7100 >> P:1.494 | V:0.030\n",
      "E3|B7200 >> P:1.383 | V:0.028\n",
      "E3|B7300 >> P:1.324 | V:0.032\n",
      "E3|B7400 >> P:1.251 | V:0.031\n",
      "E3|B7500 >> P:1.289 | V:0.034\n",
      "E3|B7600 >> P:1.283 | V:0.030\n",
      "E3|B7700 >> P:1.266 | V:0.026\n",
      "E3|B7800 >> P:1.336 | V:0.029\n",
      "E3|B7900 >> P:1.196 | V:0.031\n",
      "E3|B8000 >> P:1.280 | V:0.031\n",
      "E3|B8100 >> P:1.273 | V:0.029\n",
      "E3|B8200 >> P:1.213 | V:0.036\n",
      "E3|B8300 >> P:1.254 | V:0.025\n",
      "E3|B8400 >> P:1.221 | V:0.031\n",
      "E3|B8500 >> P:1.248 | V:0.033\n",
      "E3|B8600 >> P:1.233 | V:0.032\n",
      "E3|B8700 >> P:1.366 | V:0.035\n",
      "E3|B8800 >> P:1.369 | V:0.035\n",
      "E3|B8900 >> P:1.395 | V:0.021\n",
      "E3|B9000 >> P:1.401 | V:0.027\n",
      "E3|B9100 >> P:1.464 | V:0.030\n",
      "E3|B9200 >> P:1.340 | V:0.037\n",
      "E3|B9300 >> P:1.480 | V:0.032\n",
      "E3|B9400 >> P:1.401 | V:0.034\n",
      "E3|B9500 >> P:1.387 | V:0.030\n",
      "E3|B9600 >> P:1.387 | V:0.032\n",
      "E3|B9700 >> P:1.354 | V:0.033\n",
      "E3|B9800 >> P:1.437 | V:0.032\n",
      "E3|B9900 >> P:1.394 | V:0.027\n",
      "E3|B10000 >> P:1.433 | V:0.031\n",
      "E3|B10100 >> P:1.360 | V:0.025\n",
      "E3|B10200 >> P:1.463 | V:0.029\n",
      "E3|B10300 >> P:1.551 | V:0.036\n",
      "E3|B10400 >> P:1.355 | V:0.027\n",
      "E3|B10500 >> P:1.306 | V:0.032\n",
      "E3|B10600 >> P:1.295 | V:0.030\n",
      "E3|B10700 >> P:1.387 | V:0.030\n",
      "E3|B10800 >> P:1.405 | V:0.034\n",
      "E3|B10900 >> P:1.408 | V:0.039\n",
      "E3|B11000 >> P:1.408 | V:0.026\n",
      "E3|B11100 >> P:1.286 | V:0.032\n",
      "E3|B11200 >> P:1.253 | V:0.033\n",
      "E3|B11300 >> P:1.343 | V:0.030\n",
      "E3|B11400 >> P:1.332 | V:0.032\n",
      "E3|B11500 >> P:1.331 | V:0.034\n",
      "E3|B11600 >> P:1.431 | V:0.035\n",
      "E3|B11700 >> P:1.306 | V:0.032\n",
      "E3|B11800 >> P:1.328 | V:0.030\n",
      "E3|B11900 >> P:1.274 | V:0.029\n",
      "E3|B12000 >> P:1.373 | V:0.034\n",
      "E3|B12100 >> P:1.373 | V:0.032\n",
      "E3|B12200 >> P:1.425 | V:0.035\n",
      "E3|B12300 >> P:1.326 | V:0.032\n",
      "E3|B12400 >> P:1.350 | V:0.043\n",
      "E3|B12500 >> P:1.330 | V:0.046\n",
      "E3|B12600 >> P:1.330 | V:0.029\n",
      "E3|B12700 >> P:1.227 | V:0.033\n",
      "E3|B12800 >> P:1.371 | V:0.037\n",
      "E3|B12900 >> P:1.256 | V:0.038\n",
      "E3|B13000 >> P:1.289 | V:0.029\n",
      "E3|B13100 >> P:1.256 | V:0.031\n",
      "E3|B13200 >> P:1.337 | V:0.033\n",
      "E3|B13300 >> P:1.365 | V:0.032\n",
      "E3|B13400 >> P:1.530 | V:0.029\n",
      "E3|B13500 >> P:1.463 | V:0.033\n",
      "E3|B13600 >> P:1.457 | V:0.033\n",
      "E3|B13700 >> P:1.364 | V:0.028\n",
      "E3|B13800 >> P:1.284 | V:0.032\n",
      "E3|B13900 >> P:1.345 | V:0.033\n",
      "E3|B14000 >> P:1.378 | V:0.027\n",
      "E3|B14100 >> P:1.310 | V:0.036\n",
      "E3|B14200 >> P:1.356 | V:0.030\n",
      "E3|B14300 >> P:1.390 | V:0.034\n",
      "E3|B14400 >> P:1.315 | V:0.036\n",
      "E3|B14500 >> P:1.370 | V:0.026\n",
      "E3|B14600 >> P:1.407 | V:0.029\n",
      "E3|B14700 >> P:1.283 | V:0.026\n",
      "E3|B14800 >> P:1.318 | V:0.029\n",
      "E3|B14900 >> P:1.302 | V:0.038\n",
      "E3|B15000 >> P:1.342 | V:0.031\n",
      "E3|B15100 >> P:1.347 | V:0.025\n",
      "E3|B15200 >> P:1.402 | V:0.033\n",
      "E3|B15300 >> P:1.554 | V:0.028\n",
      "E3|B15400 >> P:1.509 | V:0.033\n",
      "E3|B15500 >> P:1.345 | V:0.024\n",
      "E3|B15600 >> P:1.395 | V:0.033\n",
      "E3|B15700 >> P:1.365 | V:0.034\n",
      "E3|B15800 >> P:1.425 | V:0.031\n",
      "E3|B15900 >> P:1.294 | V:0.029\n",
      "E3|B16000 >> P:1.394 | V:0.030\n",
      "E3|B16100 >> P:1.353 | V:0.040\n",
      "E3|B16200 >> P:1.386 | V:0.026\n",
      "E3|B16300 >> P:1.427 | V:0.029\n",
      "E3|B16400 >> P:1.447 | V:0.028\n",
      "E3|B16500 >> P:1.349 | V:0.026\n",
      "E3|B16600 >> P:1.310 | V:0.033\n",
      "E3|B16700 >> P:1.412 | V:0.027\n",
      "E3|B16800 >> P:1.349 | V:0.030\n",
      "E3|B16900 >> P:1.326 | V:0.036\n",
      "E3|B17000 >> P:1.234 | V:0.036\n",
      "E3|B17100 >> P:1.281 | V:0.030\n",
      "E3|B17200 >> P:1.372 | V:0.026\n",
      "E3|B17300 >> P:1.392 | V:0.032\n",
      "E3|B17400 >> P:1.252 | V:0.026\n",
      "E3|B17500 >> P:1.377 | V:0.028\n",
      "E3|B17600 >> P:1.291 | V:0.042\n",
      "E3|B17700 >> P:1.418 | V:0.034\n",
      "E3|B17800 >> P:1.682 | V:0.032\n",
      "E3|B17900 >> P:1.691 | V:0.011\n",
      "E3|B18000 >> P:1.669 | V:0.018\n",
      "E3|B18100 >> P:1.865 | V:0.025\n",
      "E3|B18200 >> P:1.726 | V:0.020\n",
      "E3|B18300 >> P:1.710 | V:0.025\n",
      "E3|B18400 >> P:1.651 | V:0.026\n",
      "E3|B18500 >> P:1.667 | V:0.033\n",
      "E3|B18600 >> P:1.774 | V:0.018\n",
      "E3|B18700 >> P:1.800 | V:0.025\n",
      "E3|B18800 >> P:1.738 | V:0.039\n",
      "E3|B18900 >> P:1.633 | V:0.037\n",
      "E3|B19000 >> P:1.864 | V:0.016\n",
      "E3|B19100 >> P:1.754 | V:0.022\n",
      "E3|B19200 >> P:1.736 | V:0.024\n",
      "E3|B19300 >> P:1.545 | V:0.027\n",
      "E3|B19400 >> P:1.701 | V:0.030\n",
      "E3|B19500 >> P:1.580 | V:0.030\n",
      "E3|B19600 >> P:1.755 | V:0.026\n",
      "E3|B19700 >> P:1.719 | V:0.029\n",
      "E3|B19800 >> P:1.860 | V:0.021\n",
      "E3|B19900 >> P:1.651 | V:0.013\n",
      "E3|B20000 >> P:1.814 | V:0.027\n",
      "E3|B20100 >> P:1.676 | V:0.021\n",
      "E3|B20200 >> P:1.816 | V:0.035\n",
      "E3|B20300 >> P:1.693 | V:0.021\n",
      "E3|B20400 >> P:1.703 | V:0.052\n",
      "E3|B20500 >> P:1.855 | V:0.035\n",
      "E3|B20600 >> P:1.680 | V:0.024\n",
      "E3|B20700 >> P:1.772 | V:0.016\n",
      "E3|B20800 >> P:1.635 | V:0.020\n",
      "E3|B20900 >> P:1.466 | V:0.010\n",
      "E3|B21000 >> P:1.350 | V:0.040\n",
      "E3|B21100 >> P:1.274 | V:0.031\n",
      "E3|B21200 >> P:1.313 | V:0.033\n",
      "E3|B21300 >> P:1.332 | V:0.023\n",
      "E3|B21400 >> P:1.261 | V:0.020\n",
      "E3|B21500 >> P:1.304 | V:0.036\n",
      "E3|B21600 >> P:1.223 | V:0.027\n",
      "E3|B21700 >> P:1.290 | V:0.029\n",
      "E3|B21800 >> P:1.305 | V:0.026\n",
      "‚úÖ Epoch 3 Done (469.8m). Avg Loss P:1.4422 V:0.0296\n",
      "üíæ Checkpoint Saved: resnet20_epoch3.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import chess\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURATION (ResNet-20)\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # --- ARCHITECTURE (ResNet-20) ---\n",
    "    \"num_features\": 18,\n",
    "    \"num_moves\": 4096,\n",
    "    \"num_res_blocks\": 20,       \n",
    "    \"num_channels\": 256,        # Wide layers\n",
    "\n",
    "    # --- TRAINING PARAMS ---\n",
    "    \"batch_size\": 2048,         \n",
    "    \"num_epochs\": 3,          \n",
    "    \"lr\": 0.001,                # Initial Learning Rate\n",
    "\n",
    "    # --- TIME LIMIT SAFETY ---\n",
    "    \"max_train_hours\": 12.0,    \n",
    "\n",
    "    # --- PATHS ---\n",
    "    \"data_dir\": \"/kaggle/input/chess-stockfish-data\", \n",
    "    \"save_dir\": \"./\",\n",
    "    \"resume_from\": \"/kaggle/input/teacher-model-weights/resnet20_epoch2.pth\"         \n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "print(f\"üöÄ Configuration:\")\n",
    "print(f\"   ‚Ä¢ Device: {DEVICE} ({NUM_GPUS} GPUs)\")\n",
    "print(f\"   ‚Ä¢ Dataset Root: {CONFIG['data_dir']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA ENCODING\n",
    "# ============================================================================\n",
    "PIECE_MAP = {'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5, 'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11}\n",
    "\n",
    "def encode_move(move_uci):\n",
    "    try:\n",
    "        fr = chess.parse_square(move_uci[:2])\n",
    "        to = chess.parse_square(move_uci[2:4])\n",
    "        return (fr * 64) + to\n",
    "    except: return 0\n",
    "\n",
    "def fen_to_tensor_18ch(fen):\n",
    "    parts = fen.split(' ')\n",
    "    board_str = parts[0]\n",
    "    turn = parts[1]\n",
    "    castling = parts[2]\n",
    "    try: halfmove = parts[4]\n",
    "    except: halfmove = 0\n",
    "        \n",
    "    matrix = np.zeros((18, 8, 8), dtype=np.float32)\n",
    "    rows = board_str.split('/')\n",
    "    for row_idx, row_data in enumerate(rows):\n",
    "        col_idx = 0\n",
    "        for char in row_data:\n",
    "            if char.isdigit(): col_idx += int(char)\n",
    "            else: matrix[PIECE_MAP[char], row_idx, col_idx] = 1.0; col_idx += 1\n",
    "    if 'K' in castling: matrix[12, :, :] = 1.0\n",
    "    if 'Q' in castling: matrix[13, :, :] = 1.0\n",
    "    if 'k' in castling: matrix[14, :, :] = 1.0\n",
    "    if 'q' in castling: matrix[15, :, :] = 1.0\n",
    "    if turn == 'w': matrix[16, :, :] = 1.0\n",
    "    try: matrix[17, :, :] = float(halfmove) / 100.0\n",
    "    except: pass\n",
    "    return matrix\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATASET CLASS \n",
    "# ============================================================================\n",
    "class ChessDataset18(IterableDataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.files = []\n",
    "        data_folders = glob.glob(os.path.join(root_dir, \"data_*\"))\n",
    "        \n",
    "        print(f\"üìÇ Found {len(data_folders)} data folders. Scanning for files...\")\n",
    "        \n",
    "        # 2. Recursively find all files inside these folders\n",
    "        for folder in data_folders:\n",
    "            for root, dirs, files in os.walk(folder):\n",
    "                for file in files:\n",
    "                    # Ignore hidden system files\n",
    "                    if not file.startswith('.'):\n",
    "                        self.files.append(os.path.join(root, file))\n",
    "                        \n",
    "        print(f\"‚úÖ Total Training Files Found: {len(self.files)}\")\n",
    "        if len(self.files) == 0:\n",
    "            print(\"‚ùå WARNING: No files found! Check your dataset path.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        # Shuffle files for better training distribution\n",
    "        np.random.shuffle(self.files)\n",
    "        \n",
    "        # Split work among CPU workers\n",
    "        if worker_info:\n",
    "            per_worker = int(np.ceil(len(self.files) / float(worker_info.num_workers)))\n",
    "            start = worker_info.id * per_worker\n",
    "            end = min(start + per_worker, len(self.files))\n",
    "            my_files = self.files[start:end]\n",
    "        else:\n",
    "            my_files = self.files\n",
    "            \n",
    "        for f_path in my_files:\n",
    "            try:\n",
    "                with open(f_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('|')\n",
    "                        if len(parts) < 4: continue\n",
    "                        fen, move_uci, score_str = parts[0], parts[1], parts[3]\n",
    "                        \n",
    "                        tensor = fen_to_tensor_18ch(fen)\n",
    "                        move_id = encode_move(move_uci)\n",
    "                        try: score_val = max(0.0, min(1.0, float(score_str)))\n",
    "                        except: score_val = 0.5\n",
    "                        \n",
    "                        yield tensor, move_id, score_val\n",
    "            except Exception as e:\n",
    "                # Silently skip bad files to prevent training crash\n",
    "                pass\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODEL: ResNet-20\n",
    "# ============================================================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.bn2(self.conv2(F.relu(self.bn1(self.conv1(x))))))\n",
    "\n",
    "class ChessResNet(nn.Module):\n",
    "    def __init__(self, num_features, num_moves, num_res_blocks, num_channels):\n",
    "        super().__init__()\n",
    "        self.conv_input = nn.Conv2d(num_features, num_channels, 3, padding=1, bias=False)\n",
    "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
    "        self.res_tower = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_res_blocks)])\n",
    "        self.p_conv = nn.Conv2d(num_channels, 32, 1); self.p_bn = nn.BatchNorm2d(32)\n",
    "        self.p_fc = nn.Linear(32 * 8 * 8, num_moves) \n",
    "        self.v_conv = nn.Conv2d(num_channels, 32, 1); self.v_bn = nn.BatchNorm2d(32)\n",
    "        self.v_fc1 = nn.Linear(32 * 8 * 8, 128); self.v_fc2 = nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
    "        x = self.res_tower(x)\n",
    "        p = self.p_fc(F.relu(self.p_bn(self.p_conv(x))).view(x.size(0), -1))\n",
    "        v = torch.sigmoid(self.v_fc2(F.relu(self.v_fc1(F.relu(self.v_bn(self.v_conv(x))).view(x.size(0), -1)))))\n",
    "        return p, v\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAINING LOOP\n",
    "# ============================================================================\n",
    "def train_model():\n",
    "    session_start_time = time.time()\n",
    "    \n",
    "    # 1. Init Model\n",
    "    model = ChessResNet(CONFIG[\"num_features\"], CONFIG[\"num_moves\"], \n",
    "                        CONFIG[\"num_res_blocks\"], CONFIG[\"num_channels\"])\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    if NUM_GPUS > 1:\n",
    "        print(f\"‚ö° Enabling DataParallel on {NUM_GPUS} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # 2. Resume Logic\n",
    "    start_epoch = 0\n",
    "    if CONFIG[\"resume_from\"] and os.path.exists(CONFIG[\"resume_from\"]):\n",
    "        print(f\"üîÑ Resuming from {CONFIG['resume_from']}\")\n",
    "        checkpoint = torch.load(CONFIG[\"resume_from\"], map_location=DEVICE)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(checkpoint)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        match = re.search(r\"epoch(\\d+)\", CONFIG[\"resume_from\"])\n",
    "        if match: start_epoch = int(match.group(1))\n",
    "\n",
    "    # 3. Optimizer & Data\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    criterion_p = nn.CrossEntropyLoss()\n",
    "    criterion_v = nn.MSELoss()\n",
    "    \n",
    "    # StepLR: Reduce LR every 5 epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    # Initialize Dataset with Root Directory\n",
    "    dataset = ChessDataset18(CONFIG[\"data_dir\"])\n",
    "    \n",
    "    # Num_workers=4 helps feed the 2 GPUs fast enough\n",
    "    dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], num_workers=4, pin_memory=True)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # 4. The Loop\n",
    "    for epoch in range(start_epoch, CONFIG[\"num_epochs\"]):\n",
    "        total_p_loss, total_v_loss = 0, 0\n",
    "        batch_count = 0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(f\"\\n--- Starting Epoch {epoch+1}/{CONFIG['num_epochs']} ---\")\n",
    "        \n",
    "        for boards, move_ids, scores in dataloader:\n",
    "            boards = boards.to(DEVICE)\n",
    "            move_ids = move_ids.long().to(DEVICE)\n",
    "            scores = scores.float().to(DEVICE).view(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            p_out, v_out = model(boards)\n",
    "            \n",
    "            loss_p = criterion_p(p_out, move_ids)\n",
    "            loss_v = criterion_v(v_out, scores)\n",
    "            loss = loss_p + loss_v\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_p_loss += loss_p.item()\n",
    "            total_v_loss += loss_v.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"E{epoch+1}|B{batch_count} >> P:{loss_p.item():.3f} | V:{loss_v.item():.3f}\")\n",
    "\n",
    "        # Update Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Stats\n",
    "        avg_p = total_p_loss / max(1, batch_count)\n",
    "        avg_v = total_v_loss / max(1, batch_count)\n",
    "        duration = (time.time() - epoch_start) / 60\n",
    "        print(f\"‚úÖ Epoch {epoch+1} Done ({duration:.1f}m). Avg Loss P:{avg_p:.4f} V:{avg_v:.4f}\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        save_name = f\"resnet20_epoch{epoch+1}.pth\"\n",
    "        save_path = os.path.join(CONFIG[\"save_dir\"], save_name)\n",
    "        \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), save_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        print(f\"üíæ Checkpoint Saved: {save_name}\")\n",
    "\n",
    "        # Time Check\n",
    "        elapsed_hours = (time.time() - session_start_time) / 3600\n",
    "        if elapsed_hours > CONFIG[\"max_train_hours\"]:\n",
    "            print(f\"‚ö†Ô∏è Limit Reached ({elapsed_hours:.2f}h). Stopping.\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120cadc",
   "metadata": {
    "papermill": {
     "duration": 0.009653,
     "end_time": "2026-01-25T12:11:32.213415",
     "exception": false,
     "start_time": "2026-01-25T12:11:32.203762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9311162,
     "sourceId": 14599800,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9330113,
     "sourceId": 14609190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28214.701925,
   "end_time": "2026-01-25T12:11:35.002528",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-25T04:21:20.300603",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
